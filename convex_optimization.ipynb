{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex optimization algorithms\n",
    "# Author: FÃ©lix Watine\n",
    "# Date: June 2023\n",
    "\n",
    "using LinearAlgebra\n",
    "\n",
    "# Function Derivative for f: R -> R\n",
    "\"\"\"\n",
    "    derivative(f, x)\n",
    "\n",
    "Compute the numerical derivative of a function `f` at point `x`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function for which the derivative is computed.\n",
    "- `x::Float64`: The point at which to compute the derivative.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: The numerical derivative of `f` at `x`.\n",
    "\"\"\"\n",
    "function derivative(f, x)\n",
    "    h = 0.001\n",
    "    return (f(x + h) - f(x)) / h\n",
    "end\n",
    "\n",
    "# Gradient Descent Optimization for f: R -> R\n",
    "\"\"\"\n",
    "    gradient_descent(f, k, l)\n",
    "\n",
    "Perform gradient descent to optimize function `f` with a fixed learning rate `l`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `k::Int`: Number of iterations.\n",
    "- `l::Float64`: Learning rate.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: The optimized value after `k` iterations.\n",
    "\"\"\"\n",
    "function gradient_descent(f, k, l)\n",
    "    x = 0.0\n",
    "    for i in 1:k\n",
    "        x -= l * derivative(f, x)\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Gradient of f: R^n -> R\n",
    "\"\"\"\n",
    "    gradient(f, X)\n",
    "\n",
    "Compute the gradient of a function `f` at point `X`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function for which the gradient is computed.\n",
    "- `X::Vector{Float64}`: The point at which to compute the gradient.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The gradient of `f` at `X`.\n",
    "\"\"\"\n",
    "function gradient(f, X)\n",
    "    X = Float64.(X)\n",
    "    h = 0.0001\n",
    "    n = length(X)\n",
    "    grad = zeros(n)\n",
    "    for i in 1:n\n",
    "        X_adj = copy(X)\n",
    "        X_adj[i] += h\n",
    "        grad[i] = (f(X_adj) - f(X)) / h\n",
    "    end\n",
    "    return grad\n",
    "end\n",
    "\n",
    "# Gradient Descent Optimization for f: R^n -> R\n",
    "\"\"\"\n",
    "    gradient_descent_n(f, n, k, l)\n",
    "\n",
    "Perform gradient descent for a function `f` with `n` variables.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `k::Int`: Number of iterations.\n",
    "- `l::Float64`: Learning rate.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function gradient_descent_n(f, n, k, l)\n",
    "    X = zeros(n)\n",
    "    for i in 1:k\n",
    "        X -= l * gradient(f, X)\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Gradient Descent with Optimal Step Size (not working)\n",
    "\"\"\"\n",
    "    gradient_descent_opt(f, n, k)\n",
    "\n",
    "Perform gradient descent with an optimal step size.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `k::Int`: Number of iterations.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function gradient_descent_opt(f, n, k)\n",
    "    X = ones(n)\n",
    "    for i in 1:k\n",
    "        function step_size(u)\n",
    "            return f(X - u * gradient(f, X))\n",
    "        end\n",
    "        l = gradient_descent(step_size, n, 100)\n",
    "        X -= l * gradient(f, X)\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Gradient Descent with Optimal Step Size (Armijo Rule)\n",
    "\"\"\"\n",
    "    gradient_descent_opt_2(f, n, k)\n",
    "\n",
    "Perform gradient descent using the Armijo rule for step size.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `k::Int`: Number of iterations.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function gradient_descent_opt_2(f, n, k)\n",
    "    X = ones(n)\n",
    "    t = 1.0\n",
    "    for i in 1:k\n",
    "        function h(u)\n",
    "            return f(X - u * gradient(f, X))\n",
    "        end\n",
    "        function l(u)\n",
    "            return f(X) + u * dot(gradient(f, X), -gradient(f, X))\n",
    "        end\n",
    "        while h(t) > l(0.5 * t)\n",
    "            t /= 2\n",
    "        end\n",
    "        X -= t * gradient(f, X)\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Partial Derivative of f: R^n -> R\n",
    "\"\"\"\n",
    "    partial_derivative(f, i, x)\n",
    "\n",
    "Compute the partial derivative of a function `f` with respect to `i` at point `x`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function for which the partial derivative is computed.\n",
    "- `i::Int`: The index of the partial derivative.\n",
    "- `x::Vector{Float64}`: The point at which to compute the partial derivative.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: The partial derivative of `f` with respect to `i` at `x`.\n",
    "\"\"\"\n",
    "function partial_derivative(f, i, x)\n",
    "    x = Float64.(x)\n",
    "    x_adj = copy(x)\n",
    "    h = 0.0001\n",
    "    x_adj[i] += h\n",
    "    return (f(x_adj) - f(x)) / h\n",
    "end\n",
    "\n",
    "# Hessian Matrix of f: R^n -> R\n",
    "\"\"\"\n",
    "    hessian(f, x)\n",
    "\n",
    "Compute the Hessian matrix of a function `f` at point `x`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function for which the Hessian is computed.\n",
    "- `x::Vector{Float64}`: The point at which to compute the Hessian.\n",
    "\n",
    "# Returns\n",
    "- `Matrix{Float64}`: The Hessian matrix of `f` at `x`.\n",
    "\"\"\"\n",
    "function hessian(f, x)\n",
    "    n = length(x)\n",
    "    x = Float64.(x)\n",
    "    H = zeros(n, n)\n",
    "    for i in 1:n\n",
    "        for j in 1:n\n",
    "            function first(x)\n",
    "                return partial_derivative(f, j, x)\n",
    "            end\n",
    "            H[j, i] = partial_derivative(first, i, x)\n",
    "        end\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "# Pure Newton Method\n",
    "\"\"\"\n",
    "    newton_pure(f, n, x_0, k)\n",
    "\n",
    "Perform the pure Newton method to optimize function `f`.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `x_0::Vector{Float64}`: Initial point.\n",
    "- `k::Int`: Number of iterations.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function newton_pure(f, n, x_0, k)\n",
    "    x = x_0\n",
    "    for i in 1:k\n",
    "        x -= inv(hessian(f, x)) * gradient(f, x)\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Cyclic Newton Method\n",
    "\"\"\"\n",
    "    newton_cyclic(f, n, x_0, k, m)\n",
    "\n",
    "Perform the cyclic Newton method with updated Hessian.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `x_0::Vector{Float64}`: Initial point.\n",
    "- `k::Int`: Number of iterations.\n",
    "- `m::Int`: Update frequency for the Hessian.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function newton_cyclic(f, n, x_0, k, m)\n",
    "    x = x_0\n",
    "    H_inv = inv(hessian(f, x))\n",
    "    for i in 1:k\n",
    "        x -= H_inv * gradient(f, x)\n",
    "        if i % m == 0\n",
    "            H_inv = inv(hessian(f, x))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Quasi-Newton SR1 Method\n",
    "\"\"\"\n",
    "    newton_SR1(f, n, x_0, k)\n",
    "\n",
    "Perform the SR1 quasi-Newton method for optimization.\n",
    "\n",
    "# Arguments\n",
    "- `f::Function`: The function to optimize.\n",
    "- `n::Int`: Dimensionality of the input space.\n",
    "- `x_0::Vector{Float64}`: Initial point.\n",
    "- `k::Int`: Number of iterations.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: The optimized point after `k` iterations.\n",
    "\"\"\"\n",
    "function newton_SR1(f, n, x_0, k)\n",
    "    x = x_0\n",
    "    S = Diagonal(ones(n))\n",
    "    for i in 1:k\n",
    "        x_prev = x\n",
    "        x -= S * gradient(f, x_prev)\n",
    "        delta = x - x_prev\n",
    "        gamma = gradient(f, x) - gradient(f, x_prev)\n",
    "        v = delta - S * gamma\n",
    "        denominator = dot(v, gamma)\n",
    "        if abs(denominator) < 1e-8\n",
    "            break\n",
    "        else\n",
    "            S += (v * v') / denominator\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Example Function to Optimize\n",
    "\"\"\"\n",
    "    f_2(X)\n",
    "\n",
    "Example quadratic function to optimize.\n",
    "\n",
    "# Arguments\n",
    "- `X::Vector{Float64}`: The input vector.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: The function value at `X`.\n",
    "\"\"\"\n",
    "function f_2(X)\n",
    "    A = [4 1 2; 1 5 3; 2 3 6]\n",
    "    B = [1, 1, 1]\n",
    "    return 0.5 * (X' * A * X) + B' * X\n",
    "end\n",
    "\n",
    "# Example Usage\n",
    "A = [4 1 2; 1 5 3; 2 3 6]\n",
    "B = [1 1 1]\n",
    "x_0 = zeros(3)\n",
    "\n",
    "@time println(\"Optimal X: \", inv(A) * B')\n",
    "@time println(\"X after gradient descent: \", gradient_descent_n(f_2, 3, 10, 0.1))\n",
    "@time println(\"X after gradient descent with Armijo rule: \", gradient_descent_opt_2(f_2, 3, 10))\n",
    "@time println(\"X after pure Newton method: \", newton_pure(f_2, 3, x_0, 10))\n",
    "@time println(\"X after cyclic Newton method: \", newton_cyclic(f_2, 3, x_0, 100, 10))\n",
    "@time println(\"X after SR1 quasi-Newton method: \", newton_SR1(f_2, 3, x_0, 10))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
